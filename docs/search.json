[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Meet Austin Kaduk",
    "section": "",
    "text": "Hi, I am Austin Kaduk, a fourth-year Finance student at the University of Alberta with a minor in Economics. Born and raised in Edmonton, Alberta, with family roots in Fort Smith, Northwest Territories, I bring a unique perspective shaped by my heritage and life experiences. My passion lies at the intersection of data science, statistical analysis, and finance, and I am continually enhancing my programming expertise in R and Python to unlock innovative solutions in the financial sector.\nI take immense pride in my connection to the Indigenous community, having been deeply involved through mentorship, leadership, and active participation in programs such as the First Peoples House and Braided Journeys. My work as an economics tutor has allowed me to assist and mentor other Indigenous students, fostering both academic success and a sense of empowerment.\nBeyond academics, I have developed a strong foundation in financial analysis and decision-making through hands-on roles, including a co-op internship as a Business Analyst with the City of Edmonton. My experiences have equipped me with the skills to apply strategic thinking, build impactful datasets, and craft automated workflows that streamline processes and improve outcomes.\nCreativity and strategic problem-solving define my approach to challenges. I am committed to promoting financial literacy and representation in underrepresented communities, ensuring that my work leaves a lasting positive impact. My vision is to bridge the gap between technical precision and human-centered solutions, excelling as a future CFA and banking professional while giving back to the community.\nI am always interested in connecting with like-minded individuals with a passion for growth and education. If you would like to connect, reach out to me via linkedin or email.\n\n  \n  \n    \n  \n\n  \n  \n    \n  \n\n  \n  \n    \n  \n\n\n  © , Austin Kaduk"
  },
  {
    "objectID": "projects/Austin_Kaduk.html",
    "href": "projects/Austin_Kaduk.html",
    "title": "FIN-450",
    "section": "",
    "text": "Extract the following data from FRED from 2014-01-01 to 2024-08-01.\n\nUnemployment Rate in Alaska.\nUnemployment Rate in District of Columbia.\n\nPerform the following:\n\nReturn a long data frame named unemployment with columns date, series and value.\nAmend the series names to AK and DC using tidyverse functions.\nUsing plotly, plot both time series on the same line chart. ggplot2 converted to plotly is not acceptable.\n\n\n# your code here\n\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(knitr)\nlibrary(gt)\n\ncountry &lt;- c(\"AKUR\", \"DCUR\")\n\nunemployment &lt;- country %&gt;% tidyquant::tq_get(get = \"economic.data\",\n                    from = \"2014-01-01\",\n                    to = \"2024-08-01\") %&gt;%\n  stats::na.omit()\n\nunemployment &lt;- unemployment %&gt;% \n  dplyr::mutate(series = stringr::str_replace_all(symbol, c(\"AKUR\" = \"AK\", \n                                                            \"DCUR\" = \"DC\")),\n                value = price) %&gt;%\n  dplyr::select(date, series, value) \n\nunemployment %&gt;% plotly::plot_ly(x = ~date, y = ~value, name = ~series, type = \"scatter\", mode = \"lines\")\n\n\n\n\n\n\n\n\n\n\n\nReturn a dataframe that shows the weight of each sector in the S&P 400.\nRound the weight to 3 decimals.\nSort the sector descending by weight.\n\n\n# hint: the output of your code should return a dataframe looking like the output of this one\nexample &lt;- dplyr::tibble(sector = c(\"Information Technology\",\"Consumer Discretionary\",\"Utilities\"),\n              weight = c(0.15,0.10,0.05))\n\nkable(example)\n\n\n\n\nsector\nweight\n\n\n\n\nInformation Technology\n0.15\n\n\nConsumer Discretionary\n0.10\n\n\nUtilities\n0.05\n\n\n\n\n\n\nlibrary(RTLedu)\nsp &lt;- sp400_desc\n# your code here\n\nweight &lt;- sp %&gt;% \n  dplyr::select(sector, weight) %&gt;%\n  dplyr::mutate(weight = round(weight, 3)) %&gt;%\n  dplyr::arrange(desc(weight))\nweight\n\n# A tibble: 401 × 2\n   sector                 weight\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 Health Care             0.008\n 2 Industrials             0.007\n 3 Industrials             0.007\n 4 Energy                  0.007\n 5 Industrials             0.007\n 6 Consumer Discretionary  0.006\n 7 Energy                  0.006\n 8 Materials               0.006\n 9 Information Technology  0.006\n10 Industrials             0.006\n# ℹ 391 more rows\n\n\n\n\n\nFor example, if each of the 10 largest weight companies had a weight of 1%, it would be 10%.\n\n# your code here\ntop15 &lt;- sp %&gt;%\n  select(company, weight) %&gt;%\n  dplyr::arrange(desc(weight)) %&gt;%\n  dplyr::slice(1:15) %&gt;%\n  dplyr::mutate(answer = cumsum(weight))\n\nanswer &lt;- top15 %&gt;% slice(15) %&gt;% select(answer)\ngt(answer)\n\n\n\n\n\n\n\nanswer\n\n\n\n\n0.09274607\n\n\n\n\n\n\n\n\n\n\nYou want to extract companies with the following criteria:\n\nThey are either in the Health Care OR Communication Services sector,\nAND they have a weight greater than 0.4%.\n\nCorrect the code I wrote which is not working…\n\n# leave this code as is and correct it in the next chunk\nsp400_desc %&gt;% tidyr::select(sector == \"Communication Services\" AND sector == \"Health Care\" OR weight &gt; 0.004)\n\n\n# Your corrected code here\ncorrected &lt;- sp400_desc %&gt;% dplyr::filter(sector == \"Communicaiton Services\" | sector == \"Health Care\", weight &gt; 0.004)\n\ngt(corrected)\n\n\n\n\n\n\n\nsymbol\ncompany\nidentifier\nsedol\nweight\nsector\nshares_held\nlocal_currency\n\n\n\n\nILMN\nIllumina Inc.\n452327109\n2613990\n0.007700409\nHealth Care\n1262275\nUSD\n\n\nAVTR\nAvantor Inc.\n05352A100\nBJLT387\n0.005408540\nHealth Care\n5386949\nUSD\n\n\nUTHR\nUnited Therapeutics Corporation\n91307C102\n2430412\n0.005326670\nHealth Care\n352542\nUSD\n\n\nTHC\nTenet Healthcare Corporation\n88033G407\nB8DMK08\n0.004949650\nHealth Care\n759273\nUSD\n\n\nBMRN\nBioMarin Pharmaceutical Inc.\n09061G101\n2437071\n0.004581570\nHealth Care\n1508557\nUSD\n\n\nSRPT\nSarepta Therapeutics Inc.\n803607100\nB8DPDT7\n0.004304370\nHealth Care\n755664\nUSD\n\n\n\n\n\n\n\n\n\n\n\nYou just graduated in Finance and took a job as an investment adviser for a company specializing in the real estate sector. Your company runs advertising portraying the benefit of the diversification it provides at all times versus equity indices.\nYou are skeptical.\n\nUse the following data set which represents prices of an ETF RealEstate and sp400.\nUse log() returns on for your analysis.\n\n\ncor &lt;- RTLedu::correlation %&gt;%\n  dplyr::group_by(series) %&gt;%\n  mutate(log_return = log(value / dplyr::lag(value))) %&gt;%\n  tidyr::drop_na(log_return)\n\n\n\n\n# your code here\n\ncor.roll &lt;- cor %&gt;%\n  dplyr::select(date, series, log_return) %&gt;%\n  tidyr::pivot_wider(names_from = series, values_from = log_return) %&gt;%\n  dplyr::mutate(cor60 = slider::pslide_dbl(\n    .l = list(RealEstate, sp400),\n    .f = ~ cor(.x, .y),\n    .before = 60,\n    .after = 0,\n    .complete = TRUE\n  )) %&gt;%\n  tidyr::drop_na()\n\ncor.roll %&gt;%\n  ggplot(aes(x = date, y = cor60)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"60-day Rolling Correlation\", x = \"\", y = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\nPre COVID19: 2017-2019.\nPost COVID19: 2020-now.\n\nFor full points, you must create a variable in your dataframe using dplyr::mutate() with the pre and post correlation periods (tidy workflow).\n\n# your code here\nroll_cor &lt;- cor.roll %&gt;%\n  dplyr::mutate(periods = dplyr::if_else(date &lt; \"2020-01-01\", \"Pre-COVID19\", \"Post-COVID19\")) %&gt;%\n  group_by(periods) %&gt;%\n  dplyr::summarise(avg_roll_cor = mean(cor60))\n\nkable(roll_cor, digits = 3)\n\n\n\n\nperiods\navg_roll_cor\n\n\n\n\nPost-COVID19\n0.687\n\n\nPre-COVID19\n0.490\n\n\n\n\n\n\nPre-COVID19 = 0.49, Post-COVID19 = 0.69\nPre-COVID19 = 0.52, Post-COVID19 = 0.69\nPre-COVID19 = 0.49, Post-COVID19 = 0.81\nPre-COVID19 = 0.52, Post-COVID19 = 0.81\nPre-COVID19 = 0.49, Post-COVID19 = 0.687 = 0.69\n\nyour answer\nA\n\n\n\n\nUsing the RTLedu::unemployment data set:\n\n\nIn the code chunk below: Use the feast::STL() model and plot the results using fabletools::components().\nAdd a short paragraph telling me what you observe in the change over time in their seasonality patterns.\nFrom the chart we can see that the unemployment rates in each state seem to follow a similar trend in all aspects, with Alaska having a more subtle increase and decrease in rates throughout the years, we can also see that California has the most distinct increases and decreases over the years. Observing seasonality, we can see that Alaska has the most noticeable seasonality patterns throughout the years with an interesting decrease overtime, while California and New Jersey have had a very similar pattern of seasonality. Interestingly, California and New Jersey have had an increase in seasonaly patterns throughout the years. The STL decomposition shows the effects of economic changes throughout the years through analysis of the remainder, where there was a significant jump in the unemployment rate beginning 2020 (COVID-19).\n\n# your code here\nseas &lt;- RTLedu::unemployment\n\nlibrary(fabletools)\nlibrary(feasts)\nlibrary(tsibble)\n\nseas_tsi &lt;- seas %&gt;%\n  tsibble::as_tsibble(key = state, index = date) %&gt;%\n  tsibble::index_by(freq = ~yearmonth(.)) %&gt;%\n  tsibble::group_by_key() %&gt;%\n  dplyr::summarise(\n    rate = mean(rate),\n    .groups = \"keep\"\n  ) %&gt;%\n  stats::na.omit()\n\nstl&lt;- seas_tsi %&gt;%\n  fabletools::model(feasts::STL(formula = rate ~ season(window = 13)))\n\nstl %&gt;% fabletools::components() %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\n\n\n# your code here\n\nstr_stats &lt;- seas_tsi %&gt;%\n  fabletools::features(rate, feasts::feat_stl)\nkable(str_stats, digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstate\ntrend_strength\nseasonal_strength_year\nseasonal_peak_year\nseasonal_trough_year\nspikiness\nlinearity\ncurvature\nstl_e_acf1\nstl_e_acf10\n\n\n\n\nAlaska\n0.856\n0.656\n2\n8\n0\n-0.097\n-0.084\n0.667\n0.978\n\n\nCalifornia\n0.931\n0.236\n4\n11\n0\n-0.064\n-0.204\n0.670\n0.818\n\n\nNewJersey\n0.877\n0.281\n7\n11\n0\n-0.012\n-0.165\n0.677\n0.982\n\n\n\n\n\n\n\n\n\n\n\nThis question will use the RTLedu::reg3 data set where:\n\nICLN is a clean energy ETF.\nXLE is the Energy industry ETF of the sp500 index.\nYou own ICLN in your portfolio.\nYour are interested in understanding how XLE returns explain ICLN returns.\nNo residuals or ACF tests are required for this question.\n\n\nreg1 &lt;- RTLedu::reg3\n# your code here\nlibrary(broom)\n\nfit &lt;- stats::lm(ICLN ~ XLE, reg1)\nmodel_fit &lt;- broom::tidy(fit)\n\nkable(model_fit, digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.000\n0.000\n0.453\n0.651\n\n\nXLE\n0.437\n0.016\n27.879\n0.000\n\n\n\n\nhedge_ratio &lt;- cor(reg1$ICLN, reg1$XLE) * (sd(reg1$ICLN) / sd(reg1$XLE))\n\n\nThe regression and beta (coefficient estimate) are significant.\nThe beta (coefficient estimate) is significant and the regression is not.\nTo hedge your thousand dollar investment in ICLN, you should sell approximately $450 of XLE shares.\nTo hedge your thousand dollar investment in ICLN, you should sell approximately $550 of XLE shares.\n\nyour answer(s)\nB, C\n\n\n\nA work colleague has done the regression shown below.\nHer boss knows you have a Finance background and asking you for your critical opinion.\nWrite a few bullet points summarizing your conclusions.\n\nThe Coefficient X is significant and the model explains 76.31% of variability, however the residuals tell another story.\nWe can see from the residuals vs fitted graph that there is a curved pattern, which shows clear non-linearity, meaning that the model might not fully capture the true relationship of the data, I would suggest a non-linear model, perhaps a cubic function of some sort.\nFrom the Normal Q-Q graph we can see that the residuals are not normally distributed at least near the tails of the residuals.\nThere are present observations that may be influencing the model (82, 209, 20, 19), however this should be analyzed after transforming the model.\nFrom the Breusch-Pagan Test, we can see that heteroscedasticity is present\n\n\nlibrary(ggfortify)\nreg &lt;- lm(y ~ x,data = RTLedu::reg2)\n\n\n\nRTLedu::reg2 %&gt;% ggplot(aes(x = RTLedu::reg2$x, y = RTLedu::reg2$y)) +\n  geom_point(alpha = 0.6, color = \"blue\") +\n  labs(title = \"Scatterplot of our Data\", x = \"x\", y = \"y\") +\n  theme_minimal()\n\n\n\n\n\n\n\nresults &lt;- summary(reg)\nmodel_results &lt;- broom::tidy(results)\nkable(model_results, digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.461\n0.277\n1.664\n0.099\n\n\nx\n0.677\n0.038\n17.770\n0.000\n\n\n\n\nautoplot(reg, size =0.5)\n\n\n\n\n\n\n\ntest_results &lt;- lmtest::bgtest(fit) %&gt;% broom::tidy()\nkable(test_results)\n\n\n\n\n\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n\n\n5.224175\n0.022275\n1\nBreusch-Godfrey test for serial correlation of order up to 1"
  },
  {
    "objectID": "projects/Austin_Kaduk.html#questions",
    "href": "projects/Austin_Kaduk.html#questions",
    "title": "FIN-450",
    "section": "",
    "text": "Extract the following data from FRED from 2014-01-01 to 2024-08-01.\n\nUnemployment Rate in Alaska.\nUnemployment Rate in District of Columbia.\n\nPerform the following:\n\nReturn a long data frame named unemployment with columns date, series and value.\nAmend the series names to AK and DC using tidyverse functions.\nUsing plotly, plot both time series on the same line chart. ggplot2 converted to plotly is not acceptable.\n\n\n# your code here\n\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(knitr)\nlibrary(gt)\n\ncountry &lt;- c(\"AKUR\", \"DCUR\")\n\nunemployment &lt;- country %&gt;% tidyquant::tq_get(get = \"economic.data\",\n                    from = \"2014-01-01\",\n                    to = \"2024-08-01\") %&gt;%\n  stats::na.omit()\n\nunemployment &lt;- unemployment %&gt;% \n  dplyr::mutate(series = stringr::str_replace_all(symbol, c(\"AKUR\" = \"AK\", \n                                                            \"DCUR\" = \"DC\")),\n                value = price) %&gt;%\n  dplyr::select(date, series, value) \n\nunemployment %&gt;% plotly::plot_ly(x = ~date, y = ~value, name = ~series, type = \"scatter\", mode = \"lines\")\n\n\n\n\n\n\n\n\n\n\n\nReturn a dataframe that shows the weight of each sector in the S&P 400.\nRound the weight to 3 decimals.\nSort the sector descending by weight.\n\n\n# hint: the output of your code should return a dataframe looking like the output of this one\nexample &lt;- dplyr::tibble(sector = c(\"Information Technology\",\"Consumer Discretionary\",\"Utilities\"),\n              weight = c(0.15,0.10,0.05))\n\nkable(example)\n\n\n\n\nsector\nweight\n\n\n\n\nInformation Technology\n0.15\n\n\nConsumer Discretionary\n0.10\n\n\nUtilities\n0.05\n\n\n\n\n\n\nlibrary(RTLedu)\nsp &lt;- sp400_desc\n# your code here\n\nweight &lt;- sp %&gt;% \n  dplyr::select(sector, weight) %&gt;%\n  dplyr::mutate(weight = round(weight, 3)) %&gt;%\n  dplyr::arrange(desc(weight))\nweight\n\n# A tibble: 401 × 2\n   sector                 weight\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 Health Care             0.008\n 2 Industrials             0.007\n 3 Industrials             0.007\n 4 Energy                  0.007\n 5 Industrials             0.007\n 6 Consumer Discretionary  0.006\n 7 Energy                  0.006\n 8 Materials               0.006\n 9 Information Technology  0.006\n10 Industrials             0.006\n# ℹ 391 more rows\n\n\n\n\n\nFor example, if each of the 10 largest weight companies had a weight of 1%, it would be 10%.\n\n# your code here\ntop15 &lt;- sp %&gt;%\n  select(company, weight) %&gt;%\n  dplyr::arrange(desc(weight)) %&gt;%\n  dplyr::slice(1:15) %&gt;%\n  dplyr::mutate(answer = cumsum(weight))\n\nanswer &lt;- top15 %&gt;% slice(15) %&gt;% select(answer)\ngt(answer)\n\n\n\n\n\n\n\nanswer\n\n\n\n\n0.09274607\n\n\n\n\n\n\n\n\n\n\nYou want to extract companies with the following criteria:\n\nThey are either in the Health Care OR Communication Services sector,\nAND they have a weight greater than 0.4%.\n\nCorrect the code I wrote which is not working…\n\n# leave this code as is and correct it in the next chunk\nsp400_desc %&gt;% tidyr::select(sector == \"Communication Services\" AND sector == \"Health Care\" OR weight &gt; 0.004)\n\n\n# Your corrected code here\ncorrected &lt;- sp400_desc %&gt;% dplyr::filter(sector == \"Communicaiton Services\" | sector == \"Health Care\", weight &gt; 0.004)\n\ngt(corrected)\n\n\n\n\n\n\n\nsymbol\ncompany\nidentifier\nsedol\nweight\nsector\nshares_held\nlocal_currency\n\n\n\n\nILMN\nIllumina Inc.\n452327109\n2613990\n0.007700409\nHealth Care\n1262275\nUSD\n\n\nAVTR\nAvantor Inc.\n05352A100\nBJLT387\n0.005408540\nHealth Care\n5386949\nUSD\n\n\nUTHR\nUnited Therapeutics Corporation\n91307C102\n2430412\n0.005326670\nHealth Care\n352542\nUSD\n\n\nTHC\nTenet Healthcare Corporation\n88033G407\nB8DMK08\n0.004949650\nHealth Care\n759273\nUSD\n\n\nBMRN\nBioMarin Pharmaceutical Inc.\n09061G101\n2437071\n0.004581570\nHealth Care\n1508557\nUSD\n\n\nSRPT\nSarepta Therapeutics Inc.\n803607100\nB8DPDT7\n0.004304370\nHealth Care\n755664\nUSD\n\n\n\n\n\n\n\n\n\n\n\nYou just graduated in Finance and took a job as an investment adviser for a company specializing in the real estate sector. Your company runs advertising portraying the benefit of the diversification it provides at all times versus equity indices.\nYou are skeptical.\n\nUse the following data set which represents prices of an ETF RealEstate and sp400.\nUse log() returns on for your analysis.\n\n\ncor &lt;- RTLedu::correlation %&gt;%\n  dplyr::group_by(series) %&gt;%\n  mutate(log_return = log(value / dplyr::lag(value))) %&gt;%\n  tidyr::drop_na(log_return)\n\n\n\n\n# your code here\n\ncor.roll &lt;- cor %&gt;%\n  dplyr::select(date, series, log_return) %&gt;%\n  tidyr::pivot_wider(names_from = series, values_from = log_return) %&gt;%\n  dplyr::mutate(cor60 = slider::pslide_dbl(\n    .l = list(RealEstate, sp400),\n    .f = ~ cor(.x, .y),\n    .before = 60,\n    .after = 0,\n    .complete = TRUE\n  )) %&gt;%\n  tidyr::drop_na()\n\ncor.roll %&gt;%\n  ggplot(aes(x = date, y = cor60)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"60-day Rolling Correlation\", x = \"\", y = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\nPre COVID19: 2017-2019.\nPost COVID19: 2020-now.\n\nFor full points, you must create a variable in your dataframe using dplyr::mutate() with the pre and post correlation periods (tidy workflow).\n\n# your code here\nroll_cor &lt;- cor.roll %&gt;%\n  dplyr::mutate(periods = dplyr::if_else(date &lt; \"2020-01-01\", \"Pre-COVID19\", \"Post-COVID19\")) %&gt;%\n  group_by(periods) %&gt;%\n  dplyr::summarise(avg_roll_cor = mean(cor60))\n\nkable(roll_cor, digits = 3)\n\n\n\n\nperiods\navg_roll_cor\n\n\n\n\nPost-COVID19\n0.687\n\n\nPre-COVID19\n0.490\n\n\n\n\n\n\nPre-COVID19 = 0.49, Post-COVID19 = 0.69\nPre-COVID19 = 0.52, Post-COVID19 = 0.69\nPre-COVID19 = 0.49, Post-COVID19 = 0.81\nPre-COVID19 = 0.52, Post-COVID19 = 0.81\nPre-COVID19 = 0.49, Post-COVID19 = 0.687 = 0.69\n\nyour answer\nA\n\n\n\n\nUsing the RTLedu::unemployment data set:\n\n\nIn the code chunk below: Use the feast::STL() model and plot the results using fabletools::components().\nAdd a short paragraph telling me what you observe in the change over time in their seasonality patterns.\nFrom the chart we can see that the unemployment rates in each state seem to follow a similar trend in all aspects, with Alaska having a more subtle increase and decrease in rates throughout the years, we can also see that California has the most distinct increases and decreases over the years. Observing seasonality, we can see that Alaska has the most noticeable seasonality patterns throughout the years with an interesting decrease overtime, while California and New Jersey have had a very similar pattern of seasonality. Interestingly, California and New Jersey have had an increase in seasonaly patterns throughout the years. The STL decomposition shows the effects of economic changes throughout the years through analysis of the remainder, where there was a significant jump in the unemployment rate beginning 2020 (COVID-19).\n\n# your code here\nseas &lt;- RTLedu::unemployment\n\nlibrary(fabletools)\nlibrary(feasts)\nlibrary(tsibble)\n\nseas_tsi &lt;- seas %&gt;%\n  tsibble::as_tsibble(key = state, index = date) %&gt;%\n  tsibble::index_by(freq = ~yearmonth(.)) %&gt;%\n  tsibble::group_by_key() %&gt;%\n  dplyr::summarise(\n    rate = mean(rate),\n    .groups = \"keep\"\n  ) %&gt;%\n  stats::na.omit()\n\nstl&lt;- seas_tsi %&gt;%\n  fabletools::model(feasts::STL(formula = rate ~ season(window = 13)))\n\nstl %&gt;% fabletools::components() %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\n\n\n# your code here\n\nstr_stats &lt;- seas_tsi %&gt;%\n  fabletools::features(rate, feasts::feat_stl)\nkable(str_stats, digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstate\ntrend_strength\nseasonal_strength_year\nseasonal_peak_year\nseasonal_trough_year\nspikiness\nlinearity\ncurvature\nstl_e_acf1\nstl_e_acf10\n\n\n\n\nAlaska\n0.856\n0.656\n2\n8\n0\n-0.097\n-0.084\n0.667\n0.978\n\n\nCalifornia\n0.931\n0.236\n4\n11\n0\n-0.064\n-0.204\n0.670\n0.818\n\n\nNewJersey\n0.877\n0.281\n7\n11\n0\n-0.012\n-0.165\n0.677\n0.982\n\n\n\n\n\n\n\n\n\n\n\nThis question will use the RTLedu::reg3 data set where:\n\nICLN is a clean energy ETF.\nXLE is the Energy industry ETF of the sp500 index.\nYou own ICLN in your portfolio.\nYour are interested in understanding how XLE returns explain ICLN returns.\nNo residuals or ACF tests are required for this question.\n\n\nreg1 &lt;- RTLedu::reg3\n# your code here\nlibrary(broom)\n\nfit &lt;- stats::lm(ICLN ~ XLE, reg1)\nmodel_fit &lt;- broom::tidy(fit)\n\nkable(model_fit, digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.000\n0.000\n0.453\n0.651\n\n\nXLE\n0.437\n0.016\n27.879\n0.000\n\n\n\n\nhedge_ratio &lt;- cor(reg1$ICLN, reg1$XLE) * (sd(reg1$ICLN) / sd(reg1$XLE))\n\n\nThe regression and beta (coefficient estimate) are significant.\nThe beta (coefficient estimate) is significant and the regression is not.\nTo hedge your thousand dollar investment in ICLN, you should sell approximately $450 of XLE shares.\nTo hedge your thousand dollar investment in ICLN, you should sell approximately $550 of XLE shares.\n\nyour answer(s)\nB, C\n\n\n\nA work colleague has done the regression shown below.\nHer boss knows you have a Finance background and asking you for your critical opinion.\nWrite a few bullet points summarizing your conclusions.\n\nThe Coefficient X is significant and the model explains 76.31% of variability, however the residuals tell another story.\nWe can see from the residuals vs fitted graph that there is a curved pattern, which shows clear non-linearity, meaning that the model might not fully capture the true relationship of the data, I would suggest a non-linear model, perhaps a cubic function of some sort.\nFrom the Normal Q-Q graph we can see that the residuals are not normally distributed at least near the tails of the residuals.\nThere are present observations that may be influencing the model (82, 209, 20, 19), however this should be analyzed after transforming the model.\nFrom the Breusch-Pagan Test, we can see that heteroscedasticity is present\n\n\nlibrary(ggfortify)\nreg &lt;- lm(y ~ x,data = RTLedu::reg2)\n\n\n\nRTLedu::reg2 %&gt;% ggplot(aes(x = RTLedu::reg2$x, y = RTLedu::reg2$y)) +\n  geom_point(alpha = 0.6, color = \"blue\") +\n  labs(title = \"Scatterplot of our Data\", x = \"x\", y = \"y\") +\n  theme_minimal()\n\n\n\n\n\n\n\nresults &lt;- summary(reg)\nmodel_results &lt;- broom::tidy(results)\nkable(model_results, digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.461\n0.277\n1.664\n0.099\n\n\nx\n0.677\n0.038\n17.770\n0.000\n\n\n\n\nautoplot(reg, size =0.5)\n\n\n\n\n\n\n\ntest_results &lt;- lmtest::bgtest(fit) %&gt;% broom::tidy()\nkable(test_results)\n\n\n\n\n\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n\n\n5.224175\n0.022275\n1\nBreusch-Godfrey test for serial correlation of order up to 1"
  },
  {
    "objectID": "projects/IR_risk_Analysis.html",
    "href": "projects/IR_risk_Analysis.html",
    "title": "Practical IR Portfolio Sensitivities",
    "section": "",
    "text": "Description\nImagine you are a portfolio manager at a fixed-income investment firm. Your primary responsibility is to manage interest rate risk across a diverse bond portfolio. You are tasked with analyzing how various interest rate movements impact your portfolio’s value and using quantitative tools to estimate, visualize, and interpret these risks. Your goal is to provide insights that will help guide strategic decisions for hedging and optimizing the portfolio.\nAs a Finance graduate, you will draw on the skills you have learned in fixed income valuation, and be expected to use current data for interest rates in the US Treasury market. You will need to round you the YTMs and Coupons of each bond to the last business day of the month prior to this assigment being given to you.\n\n\nObjective\nYou will recreate numerical sensitivities (Delta and Gamma) for specific bond positions and assess how changes in interest rates affect the portfolio’s P&L. The focus will be on both individual bonds and the overall portfolio, with practical applications for risk management.\n\n\nExposure\nThe bond positions you have are:\n\nLong $1,000,000 a 10-year bond with 3% coupon and an initial Yield To Maturity (YTM) of 3%.\nShort $500,000 a 2-year bond with 3% coupon and an initial YTM of 3%.\nShort $500,000 a 30-year bond with 3% coupon and an initial YTM of 3%.\n\nYou may use the RTL::bond() pricing function.\n\n\nQuestion 1 (9 points)\n\nRecreate the numerical sensitivities (delta/gamma PL est) for each portfolio positions.\n\nAll YTMs are semi-annual.\nThe step size for bonds is one basis point using the central difference method.\n\nCreate plots showing:\n\nThe x-axis YTMs from zero to 6% and the y-axis showing the Bond Price\nA line showing the delta approximation price\nA line showing the actual price\n\n\n\n\nWarning in geom_text(aes(x = 0.031, label = \"Coupon Rate\", y = 150), colour = \"red\", : All aesthetics have length 1, but the data has 1803 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2 (3 points)\nNow show a chart of portfolio change in price (y-axis) vs YTM (x-axis) for \\(\\pm\\) 3% change in YTM.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3 (4 points)\nShow the initial delta (stepsize = 1 bp, central difference) of the portfolio.\n\n\n\n\n\n\n\n\nPortfolioDelta\n\n\n\n\n222.4354\n\n\n\n\n\n\n\n\n\nQuestion 4 (8 points)\nShow a delta/gamma PL estimate vs actual PL when YTMs change as follows:\n\n10-year bond YTM up by 75 bps.\n2-year bond YTM up by 50 bps.\n30-year bond YTM up by 100 bps.\n\n\n\n\n\n\n\n\n\nBond\nMaturity\nActualPL\nDeltaPL\nGammaPL\nPLEst\nPLUnattributed\n\n\n\n\n1\n10\n−$62,064.02\n−$64,382.41\n$2,378.75\n−$62,003.65\n−$60.37\n\n\n2\n2\n$4,788.68\n$4,817.98\n−$29.44\n$4,788.54\n$0.14\n\n\n3\n30\n$86,902.22\n$98,450.79\n−$12,636.73\n$85,814.06\n$1,088.16\n\n\nTotal\n\n$29,626.87\n$38,886.37\n−$10,287.42\n$28,598.95\n$1,027.93\n\n\n\n\n\n\n\n\n\nQuestion 5 (6 points)\nDiscuss the learnings and insights in IR valuation and risk management. You explanation must: + Tell me something valuable from a delta and gamma risk perspectives, and + Have a focus on Pay particular attention to real world bond markets versus textbook examples.\nDelta measures the sensitivity of a bond or portfolio of bonds’ price to small changes in yield. In textbook explanations, delta is assumed to have a linear relationship between price and yield, but in the real world, this assumption is less helpful for larger yield movements because the price to yield relationship is convex. This is where gamma is important, gamma measures the rate at which delta changes as yields fluctuate. If gamma is high, delta is unstable, which means that hedging strategies that are based solely on delta become ineffective as interest rates shift.\nIn our portfolio, we are dollar-neutral but not risk-neutral. While the value of our positions sums to zero, we are exposed to risk due to the different maturities and convexities of the bonds. Long-term bonds have a more extreme convexity profile. Our portfolio’s delta flips from long to short ~ 4.75% YTM because as yields rise, the short 30-year bond, having the highest convexity, dominates the overall risk profile. Meanwhile, the shorter-term bonds (2-year and 10-year) lose influence more quickly. Making gamma an essential factor in our P&L attribution analysis.\nHedging purely based on dollar value and/or delta exposes the portfolio to second-order risks, which can lead to significant P&L attribution errors during non-parallel changes in the yield curve. When looking for real-world examples, I found that the LTCM collapse in 1998 was a good example of how a theoretically neutral strategy can dissolve when gamma exposure is ignored. They bet on convergence trades in fixed income, believing that bond spreads would narrow over time. While they were dollar-neutral and duration-neutral, they had significant basis risk and tail risk, meaning they were short gamma with respect to spread widening, making them vulnerable to changing market conditions. The Asian/Russian crises of 1997-98 caused yields to fall while swap yields and off-the-run yields did not fall as much. The difference moved against them massively, and they ended up with a $4 billion loss in weeks; with around $1.6 billion due to the swap spread widening.\nManaging risk purely through duration (delta) is insufficient and incorporating convexity (gamma) helps ensure that a portfolio remains stable during market shifts. It is important as a risk manager to incorporate other tools such as swaps which are typically used for adjusting directional delta exposure, bond futures that provide liquidity for managing rate risk, and options (swaptions) that are effective for hedging gamma risk but can be expensive in volatile environments.\nThe key takeaway I got from this project is that simply being dollar-neutral does not mean being risk-neutral. While delta explains first-order price changes, gamma determines how those price sensitivities evolve, making it one of the most crucial considerations in portfolio risk management, especially when dealing with differing convexity profiles like the one we analyzed.\nIn regards to Vega, “Speed”, and higher order derivatives. Vega risk is essentially zero in a straight bond portfolio because the cash flows are fixed with no volatility parameter affecting their value like an option has. However, in bonds with embedded options, such as callable bonds or MBS, Vega risk can arise. For bonds, gamma changes gradually as a bond ages or yields move. This is why third-order effects like “Speed” (the rate of change of gamma) are extremely small relative to Delta and Gamma."
  },
  {
    "objectID": "projects/Assessing_fitness.html",
    "href": "projects/Assessing_fitness.html",
    "title": "Assessing Fitness",
    "section": "",
    "text": "We have identified five critical variables for measuring fitness: average pace, average heart rate, distance, total ascent and aerobic total expenditure.\nBoth the runners show positive trends overall, with Collie excelling in technical efficiency and consistent progression like maintaining better speed, stride length and aerobic total expenditure, whereas Hound shows higher energy output with varying improvements for cumulative distance and energy expenditure over the years.\nCollie does well in technical metrics like aerobic total expenditure, average speed, average cadence, stride length, with consistent training patterns and steady elevation gains, focused on trail running with lower injury risk.\nHound has higher total distance and caloric expenditure with variable training intensities and stronger peak performances, focused on track and street running with higher injury risk.\nWe recommend Hound for coaching for fitness, since he shown greater optimization potential, strong adaptation skill set, and better room for structured growth through periodization training and recovery management."
  },
  {
    "objectID": "projects/Assessing_fitness.html#distributions",
    "href": "projects/Assessing_fitness.html#distributions",
    "title": "Assessing Fitness",
    "section": "Distributions",
    "text": "Distributions\nNext we are going to analyze the distributions of our variables in order to capture if there is any abnormalities.\n\n\n\n\n\n\n\n\n\nFrom the analysis of our distributions, we can clearly see that we are working with data from two different runners, with a noticeable bi-modal relationship in elevation, average heart rate, max run cadence, and stride length. We can also see that there are significantly right-skewed variables especially those related to distance, time, elevation, and calories. Indicating that while most activities are moderate intensity, a few activities are significantly longer, higher in elevation, or more intense.\nAn important thing to note is the presence of outliers in our data, which suggest different running conditions or exceptional efforts being made by the runners in our data."
  },
  {
    "objectID": "projects/Assessing_fitness.html#how-does-our-data-behave",
    "href": "projects/Assessing_fitness.html#how-does-our-data-behave",
    "title": "Assessing Fitness",
    "section": "How does our data behave?",
    "text": "How does our data behave?\nHere we are trying to understand how our data behaves, specifically we are trying to see if our data exhibits stationarity, seasonality, multicollinearity, and normality.\n\nSummary of all tests P-values:\n\n\n          Variable Normal_p_value adf_p_value KP_Level_p_value KP_trend_p_value\n        aerobic_te         0.0033      0.0185           0.0100           0.0397\n            avg_hr         0.0000      0.0100           0.1000           0.0100\n          avg_pace         0.0000      0.0212           0.0100           0.0100\n   avg_run_cadence         0.0000      0.0100           0.0100           0.0100\n avg_stride_length         0.1978      0.0228           0.0100           0.0100\n         best_pace         0.0000      0.0100           0.0100           0.0100\n          calories         0.0000      0.0100           0.0100           0.0199\n          distance         0.0000      0.0100           0.0100           0.1000\n      elapsed_time         0.0000      0.0100           0.0100           0.0242\n            max_hr         0.0004      0.0761           0.0281           0.0100\n   max_run_cadence         0.0000      0.0100           0.0100           0.1000\n       moving_time         0.0000      0.0100           0.0100           0.0247\n      total_ascent         0.0000      0.0100           0.1000           0.1000\n     total_descent         0.0000      0.0100           0.1000           0.1000\n\n\nSince our Jarque-Bera test results show a low p-value for all variables except for Average Stride Length, we must reject the null that our data is normally distributed, and assume it is not normal. However, after analysis of our Correlation Matrix, we are making the decision to not use the following variables in our regression analysis: Average Stride Length, Moving Time, Max Heart Rate, Best Pace, and Maximum Run Cadence.\nGiven that our results from ADF indicate that the series could be stationary while the KPSS test suggests that non-stationarity remains, especially within the trend component. It is clear we are going to need to do further analysis on trend and seasonality."
  },
  {
    "objectID": "projects/Assessing_fitness.html#understanding-our-remaining-variables",
    "href": "projects/Assessing_fitness.html#understanding-our-remaining-variables",
    "title": "Assessing Fitness",
    "section": "Understanding our remaining variables:",
    "text": "Understanding our remaining variables:\n\n\n\n\n\n\n\n\n\nThis pairwise correlation plot provides a more detailed overview of key running metrics interact with each other, highlighting relationships that can inform performance evaluation and training strategies. Some of the most important insights that we can gain from this chart are the following:\nThere are positive relationships between the following pairs:\nDistance and Calories (0.935), Total Ascent and Distance (0.476), Total Ascent and Total Descent (0.851)\nWhat we can gain from understanding these positive relationships is that as a runner covers more distance, calorie expenditure increases proportionally. When our runners take on more elevation gain, they also tend to cover longer distances. Suggesting a pattern of tackling challenging terrains during longer runs. Lastly, we can analyze that our runners typically are returning to their starting elevation, which is typical in looped or hiking trails.\nThere are interesting negative correlations between the following pairs:\nAverage Stride Length and Total Descent (-0.648), Aerobic Training Effect and Average Pace (-0.299)\nWe can also understand that our runners typically are taking shorter steps when going downhill, likely to maintain control and balance. There is also a noticeable effect on our runners pace when they engage into more intense training sessions."
  },
  {
    "objectID": "projects/Assessing_fitness.html#seasonality",
    "href": "projects/Assessing_fitness.html#seasonality",
    "title": "Assessing Fitness",
    "section": "Seasonality:",
    "text": "Seasonality:\nNow that we have a more clear picture on Stationarity, Multicolinearity, and Normality. It is time for us to understand the seasonality of our variables, for this analysis, we have decided to analyze: Distance, Total Ascent, Average Pace, Average Run Cadence, and Aerobic TE.\n\n\n\n\n\n\n\n\n\nTrend: The upward trend indicates that our runner’s have a steady improvement in endurance, as they are consistently able to cover more distance over time. This can be seen as a positive indicator of increased cardiovascular fitness.\nSeasonality: There is a recurring seasonal pattern where distance increases and decreases at specific intervals, more specifically our runners cover more distance in the summer months and less distance in the winter months.\nResiduals: The occasional outliers indicate that our runners may have had some periods of exceptional performance or periods where external factors may have impacted the runner’s performance.\n\n\n\n\n\n\n\n\n\nTrend: The trend suggests that our runners are gradually incorporating more challenging routes over time, this is important because it improves muscle strength and endurance.\nSeasonality: There are visible seasonal peaks, which may correspond to our runners favoring trail running during certain times of the year, the most noticeable peaks are nearest to the middle of the year (summer time).\nResiduals: Outliers in the residuals are likely capturing exceptional efforts on more challenging terrain (i.e Mountain climbing)\n\n\n\n\n\n\n\n\n\nTrend: The overall trend tells us that our runners have had a gradual decrease in pace, indicating that they are getting faster over time.\nSeasonality: Pace is fluctuating in a predictable pattern, with noticeable decreases in the summer time and increases near winter. This makes sense because our runners are likely not performing races in the winter time and they have a much better environment to run in without snow.\nResiduals: Here the residuals capture moments of unusually fast or slow performances, indicating that our runners have performed in races and or challenging terrain.\n\n\n\n\n\n\n\n\n\nTrend: The Trend shows a slow upward movement, which suggests that our runners are slightly increasing their cadence overtime. This is good since a higher cadence generally reflects improved running efficiency and overall better running mechanics.\nSeasonality: There is a repeating cycle in cadence, explaining that our runners undertake periods of more intense and structured training in the winter months. Suggesting that our runners gain a greater focus on improving running mechanics to pursue more intense events, such as races or hiking.\nResiduals: Highlighting the irregularities in cadence during mid years show interesting information, we can see that our runners did better in their training in 2021 compared to 2022.\n\n\n\n\n\n\n\n\n\nTrend: The trend shows that our runners are sustaining high levels of cardiovascular effort over time, which suggests that aerobic capacity is being steadily challenged and improved.\nSeasonality: The seasonal component reflects variations in the intensity of training. We can see that there is clear periods of high-intensity training in the summer time, with drop offs after followed by increases back to the peak indicates that our runners may be training for a race or taking it easy until the weather clears up.\nResiduals: The outliers in the residuals show us where the training effect was much high or lower than expected, indicating either highly intense efforts or light recovery runs."
  },
  {
    "objectID": "projects/Assessing_fitness.html#taking-a-look-at-how-our-runners-perform-individually",
    "href": "projects/Assessing_fitness.html#taking-a-look-at-how-our-runners-perform-individually",
    "title": "Assessing Fitness",
    "section": "Taking a look at how our Runners perform individually",
    "text": "Taking a look at how our Runners perform individually\n\nOver the years:\n\n\n\n\n\n\n\n\n\nWe can see from our analysis of Calories and Distance that Hound tends to have a more variable running pattern, with periods of high-intensity or longer runs mixed with lower-intensity periods. While Collie, likes to focus on a more steady and consistent running regimen. The spikes in Hound’s charts suggest that Hound is more likely to engage in intensive training, which can indicate that they would enjoy a strategy focused on periodic intense efforts. While Collie’s consistency can reflect a more sustainable or gradual training approach.\n\n\n\n\n\n\n\n\n\nCollie continues to appear to have a consistent effort when it comes to elevation gain, consistently accumulating elevation, suggesting that they enjoy more long-term endurance training on hilly or elevated routes, this is also clear when we analyze their Activity types, Collie predominantly enjoys trail running over anything else. While Hound tends to engage in more intense efforts for higher ascent in individual runs, overall, these efforts haven’t resulted in as much elevation gain over time as Collie’s more consistent approach. Examining Hounds activity types, it is clear that Hound doesn’t engage in trail running as much as Collie, they prefer to do more track running, street running, and work on the treadmill.\nIt is clear that Collie has an advantage in terms of total ascent, indicating that a steady focus on elevation during runs would be a good way to coach them. While Hound may enjoy a more interval-based approach to training on inclines.\n\n\n\n\n\n\n\n\n\nWith the analysis of speed and Aerobic training effect, we can see that Collie remains consistently achieving a higher TE over the years compared to Hound with fewer fluctuations. This is more pronounced when we get the cumulative total of aerobic TE, suggesting that Collie has a more moderate and consistent level of aerobic challenge throughout his training sessions. When compared to Hound who has a more variable yet gradual increase of cumulative aerobic TE.\nWhen taking a look at the pace and speed of our runners, there is an interesting insight showing that despite Collie’s approach to more trail running, they maintain a lower pace than Hound on average and this translates into a higher speed as shown in the above chart.\n\n\nPerformance Overall\n\n\n\n\n\n\n\n\n\nTaking a look at the overall fitness metrics of each runner:\n\nCollie overall outperforms Hound in key fitness metrics such as Aerobic TE, Average Speed, Run Cadence, and Stride Length, which are the most critical indicators of overall running fitness, cardiovascular endurance, and efficiency.\nHound overall excels in Calories burned and distance traveled, suggesting a higher energy output and endurance compared to Collie, but it is clear that Hound might benefit from a more structured training focused on aerobic improvement and speed to match Collie’s overall fitness levels.\n\n\n\nPerformance Over the years\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that throughout the years Collie has maintained a better speed, stride length, and higher Aerobic TE training. However, It seems as if Hound has maintained an overall higher energy expenditure and has had slightly more distance traveled than Collie, with the exception of 2022 where it is observed that Collie was taking it more easy with their running activities.\nIn terms of improvement, it is clear that both Collie and Hound have made improvements to both their efficiency, running mechanics, and endurance. The abilities in both of our runners are clearly getting better and they both have a dedication to training albeit one is more consistent in terms of Aerobic TE and growth"
  },
  {
    "objectID": "projects/Assessing_fitness.html#how-does-our-model-perform",
    "href": "projects/Assessing_fitness.html#how-does-our-model-perform",
    "title": "Assessing Fitness",
    "section": "How does our Model Perform?",
    "text": "How does our Model Perform?\nWe have decided to make our model based off of Aerobic TE being the dependent variable and with Average Pace, Average Heart rate, distance, and total ascent being the independent variables. We will be running our models based on this order in terms of \\(R^2\\) (highest to lowest), here are the following results.\n\n\n# A tibble: 4 × 5\n  model                                   output r.squared adj.r.squared p.value\n  &lt;chr&gt;                                   &lt;list&gt;     &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1 aerobic_te ~ avg_pace                   &lt;lm&gt;       0.271         0.269       0\n2 aerobic_te ~ avg_pace + avg_hr          &lt;lm&gt;       0.437         0.432       0\n3 aerobic_te ~ avg_pace + avg_hr + dista… &lt;lm&gt;       0.582         0.577       0\n4 aerobic_te ~ avg_pace + avg_hr + dista… &lt;lm&gt;       0.618         0.612       0\n\n\nWe can see that all of our models have significance, however the final model including all of our variables have the highest explanation of \\(R^2_{adjusted}\\) we can take a closer look at our final model to see if there is the presence of multicolinearity, if our terms are significant, and if our Residuals follow the assumptions of a liner regression.\n\n\n# A tibble: 5 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -0.155    0.579       -0.267 7.89e- 1\n2 avg_pace     -0.00716  0.000527   -13.6   3.27e-32\n3 avg_hr        0.0419   0.00363     11.6   2.82e-25\n4 distance      0.0428   0.00582      7.35  2.52e-12\n5 total_ascent  0.00184  0.000366     5.02  9.71e- 7\n\n\nTaking a look at the P-values of our predictors we can see that they all hold statistical significance, meaning they all contribute meaningfully to predicting Aerobic TE. The key Findings of this model suggest that increasing distance, heart rate, and elevation gain (total ascent) during training can enhance Aerobic training effects, while having slower paces can reduce it.\n\n\n# A tibble: 4 × 2\n  Predictors     VIF\n  &lt;chr&gt;        &lt;dbl&gt;\n1 avg_pace      1.22\n2 avg_hr        1.06\n3 distance      1.19\n4 total_ascent  1.43\n\n\nJudging from our VIF results we can see that our model seems to have very low multicollinearity; these values lay within the boundary of 1 &lt; VIF &lt; 5, which is within the typical acceptable limits and are generally not problematic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing our residuals gives us some insights to our model, we can see that with the curved pattern in the residuals vs. fitted plot, our model may benefit from polynomial or interaction terms. In the Q-Q plot we can see that for the most part our residuals are normally distributed with some drifting at the tails. The Scale-location plot shows that we have some heteroscedasticity in our model, and that there are a couple of influential points effecting our model.\nAnalyzing the specific points in the model shows that they are not outstanding outliers, they actually rather serve to explain the nature of Aerobic TE and how it can be skewed depending on a multitude of factors not explained in our model and data, such as hydration, sleep quality, and nutrition on the day of the run."
  },
  {
    "objectID": "projects/Assessing_fitness.html#recommendation",
    "href": "projects/Assessing_fitness.html#recommendation",
    "title": "Assessing Fitness",
    "section": "Recommendation",
    "text": "Recommendation\nBased on our model, we would like to implement a training strategy that focuses on increasing distance, heart rate, elevation gain, while maintaining a faster pace. In the efforts to maximize aerobic training effect, which should improve overall fitness over time.\nHound shows variable training patterns compared to Collie. There are periods of high intensity workouts and low intensity workouts. Collie, on the other hand, shows sustained training progression. As mentioned earlier, this means that if you are looking to coach someone for high performance, Hound is a better candidate because he shows strong adaptation to intensive training and can handle different training loads. Downside of this is this indicates a higher injury risk for variable intensity workouts.\nCollie has better baseline maintenance, consistent, steady training, and progression patterns. This is reflected through his consistent heart rates adhering to similar distances. This implies that he has consistent fitness levels throughout the years.\nHound has higher caloric expenditure by distance that implies greater work intensity in his workouts which also reflects his higher heart rate. On the other hand, Collie has consistent energy usage in his workouts.\nOn the basis of these conclusions, Hound is a viable option for coaching over Collie. Primarily, because Hound’s variable patterns and peak loads in fitness parameters could be optimized to result in consistent fitness levels. This can be done through structured coaching in a more controlled environment, which Hound prefers after analyzing his main activity types. We expect to see more improvement in Hound’s overall fitness improvements than Collie’s as a result of this. Not to say that Collie cannot be coached, however, there is major room for growth from Hound."
  },
  {
    "objectID": "projects/NPV@RISK_Analysis.html#historical-trend-distribution-analysis",
    "href": "projects/NPV@RISK_Analysis.html#historical-trend-distribution-analysis",
    "title": "A Desicion Towards Independence (NPV@RISK)",
    "section": "Historical Trend & Distribution Analysis",
    "text": "Historical Trend & Distribution Analysis\nIn November 2018, U.S. oil prices plunged below $50 per barrel, falling over 30% since October due to rising supply and fears of weakening demand. President Donald Trump praised Saudi Arabia for keeping prices low, while concerns grew over U.S. shale oversupply, OPEC production uncertainty, and geopolitical tensions. This market instability is reflected in the CL-SYN spread, which shows increased volatility during this period, highlighting the impact of global supply-demand shifts on regional crude pricing. (CNN)\nThe 2021–2023 global energy crisis was driven by post-pandemic supply shortages, coal trade disputes (China-Australia), climate-related hydropower declines, and geopolitical tensions like Russia’s invasion of Ukraine, which led to sanctions and disrupted oil and gas supplies. OPEC+ further tightened markets with production cuts in October 2022, intensifying price volatility and global energy shortages. This aligns with the CL-SYN spread surge in 2022, where prices spiked above $20/barrel, reflecting the market’s reaction to these supply constraints and geopolitical disruptions. (Wikipedia)\nThe CL-SYN spread distribution is right-skewed, with most values between CAD 10-15 per spread in barrels. With a skewness of 0.63 illustrating that majority of the historical observation has been towards the lower end of the spread. It means that majority of the time spread changes within lower end. Reflecting a typical premium for synthetic crude, this aligns with the observed mean of $16.47 per barrel, and extreme positives (up to CAD 35) align with major global events, such as the 2018 oil price crash, the 2020 COVID-19 collapse, and the 2021-2023 energy crisis, showing how external shocks drive volatility in the spread we are trying to capture.\nOverall, the CL-SYN spread shows signs of mean reversion, despite extreme deviations during global crises. Suggesting that while short-term shocks impact the spread, it tends to revert towards a stable range over time. Based on this analysis, we came to the conclusion to fit an Ornstein-Uhlenbeck Mean-reversion with Jumps model to effectively evaluate the value of our project decision."
  },
  {
    "objectID": "projects/NPV@RISK_Analysis.html#simulated-spread",
    "href": "projects/NPV@RISK_Analysis.html#simulated-spread",
    "title": "A Desicion Towards Independence (NPV@RISK)",
    "section": "Simulated Spread",
    "text": "Simulated Spread\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheta\nmu\nsigma\nhalfLife\njump_prob\njump_avesize\njump_stdv\n\n\n\n\n1.538397\n17.34466\n9.539241\n0.4505645\n0.0493827\n9.491148\n1.613076\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJustification of choice of parameters\nMu (Long-Run Mean): Our historical data exhibits a mean spread of 16.47, while our model estimates 17.34, showing a clear consistency with the observed data. This suggests our model effectively captures the central tendency of the spread.\nTheta (Mean-Reversion Speed): With a moderate theta of 1.54, the spread reverts to its mean approximately every 1.5 years. This reflects a realistic pace of reversion, balancing short-term fluctuations with long-term stability.\nSigma (Volatility): The estimated volatility of 9.53 suggests that while fluctuations occur, they are within a controlled range, capturing the moderate yet dynamic nature of the spread.\nJump Probability & Size: The model detects jumps occurring approximately 4.9% of the time. The average jump size is 9.49 dollars, aligning with observed market shocks, and the estimated jump standard deviation of 1.61 captures variability in jump magnitudes.\nHalf-Life: The spread takes approximately 0.45 years to close half the gap between its current level and the long-term mean. This estimation aligns well with market behavior, reinforcing the model’s ability to reflect real-world reversion dynamics.\nOverall, our Ornstein-Uhlenbeck with Jumps model appears to capture the spread’s mean-reverting nature while allowing for occasional significant deviations. The spread operates within a band of approximately -$8 to 42$/barrel, showing expected stochastic behavior with jumps, yet maintaining a stable long-term range, allowing our model to effectively account for uncertainty within the markets."
  },
  {
    "objectID": "projects/NPV@RISK_Analysis.html#npvrisk-analysis",
    "href": "projects/NPV@RISK_Analysis.html#npvrisk-analysis",
    "title": "A Desicion Towards Independence (NPV@RISK)",
    "section": "NPV@Risk Analysis",
    "text": "NPV@Risk Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNPV Risk Metrics\n\n\nMonte Carlo Simulation Results\n\n\nRisk Metric\nValue\n\n\n\n\nExpected NPV ($M)\n42.16\n\n\nStandard Deviation ($M)\n645.76\n\n\nVaR (95%) ($M)\n−1,016.74\n\n\nProbability of Negative NPV (%)\n48.60%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSensitivity Analysis\nWith 500 simulations for analysis, we get an expected (average) mean of $42 million. When increasing the simulation to 5000 the expected NPV decreased to -$47 million. With more simulations, the simulation distribution becomes more skewed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom our sensitivity analysis, we observe that the minimum operating threshold for operating cost should be 11 dollars per barrel (CL_SYN spread), because that is where the profit is maximized or expected NPV is highest. Also, the probability of negative NPV is 100% at the minimum operating threshold of $20 and lowest at $11. This solidifies the target minimum operating threshold to be $11.\nHigher expansion rate means expansion occurs more often in the simulations. Indicating that there is a positive relationship between higher expansion rate and higher NPV. However the sweet spot is around $20 to $25, that is, NPV changes the greatest and there is minimum change in expansion rate. A Higher expansion threshold lowers the expansion rate, and vice versa (as a result a lower NPV).\nAssuming that fixed costs and initial expenditure are predetermined or sunk cost we did not do a sensitivity analysis on them. It is with the assumption that the primary goal of a sensitivity analysis is to assess how changes in uncertain, variable factors—such as sales volume, selling price, or variable costs—affect the project’s outcomes. Since fixed costs and initial expenditures are usually contractual or one-time outlays, they’re assumed to remain unchanged unless we explicitly want to test scenarios where these assumptions might vary.\n\n\n\n\n\n\n\n\n\nThe tornado chart highlights that Operating Threshold (X) - Low has the most significant negative impact, driving NPV down by over $1.5 billion, as running operations at an unprofitable threshold leads to major losses. Expansion Threshold (Ci) - Low also reduces NPV, indicating that premature expansion increases costs without guaranteeing returns. Conversely, Expansion Threshold (Ci) - High is the only factor that increases NPV, showing that a more conservative expansion strategy results in better financial outcomes. These insights reinforce that setting an optimal Operating Threshold (X) is critical, as poor calibration can lead to extreme downside risk, while strategic expansion decisions help maximize profitability."
  },
  {
    "objectID": "projects/NPV@RISK_Analysis.html#strategic-recommendation",
    "href": "projects/NPV@RISK_Analysis.html#strategic-recommendation",
    "title": "A Desicion Towards Independence (NPV@RISK)",
    "section": "Strategic Recommendation",
    "text": "Strategic Recommendation\nBased on our analysis, we recommend proceeding with the project. However, there are clear opportunities to enhance profitability by refining key constraints identified in our evaluation.\nCurrently, the option to expand the upgrader is only available at t = 5, resembling a European call option. However, the existing expansion threshold of $25/barrel creates a significant limitation on potential returns. To maximize value, we recommend lowering this threshold to approximately $20/barrel, enabling optimal expansion and greater upside capture in favorable market conditions. This adjustment aligns with our NPV analysis and could significantly improve the project’s financial outcome.\n\n\n\nComparison of Traditional NPV vs. NPV@Risk\n\n\nMethod\nNPV Value ($M)\n\n\n\n\nTraditional NPV\n-55.34\n\n\nNPV@Risk (Mean)\n42.16\n\n\n\n\n\n\n\nThe Traditional NPV approach yields a negative value of -$55 million, suggesting that the project is not feasible under a rigid, static framework. In contrast, the NPV@Risk approach, which incorporates flexibility and uncertainty, results in a positive expected NPV of $42 million, demonstrating the significant value of adaptive decision-making. The differences between the two approaches highlight the clear limitations of a traditional NPV method, which fails to capture flexibility and changing market dynamics. While a traditional NPV approach can be served useful in stable, predictable environments (e.g regulated utilities), it struggles in more volatile markets (e.g commodities or energy), where price fluctuations and strategic adjustments play a more critical role. By incorporating NPV@Risk and real options analysis, firms can account for uncertainty, adaptability, and decision-making flexibility, ultimately leading to a more realistic valuation of their projects and more clear insights on the risks that come with it.\nBeyond the limitations of the traditional NPV approach, our analysis further explores how different levels of flexibility impact project value.\n\nThe value of operational flexibility and expansion\n\n\n\nExpected NPV and Option Value for different flexibility levels\n\n\nFlexibility\nExpected NPV ($M)\nOption Value ($M)\n\n\n\n\nFull Flexibility\n42.16\n247.69\n\n\nExpansion Only\n8.10\n34.06\n\n\nShutdown Flexibility\n-205.53\n213.63\n\n\n\n\n\n\n\nThe real options embedded in our NPV model highlight the substantial financial impact of managerial flexibility under uncertainty. Without any flexibility, where the project must always operate regardless of market conditions, the expected NPV is -$205.53 million, reflecting the significant downside risk. Allowing only the option to shut down (but not expand) improves this outcome dramatically to $8.10 million, demonstrating the importance of avoiding prolonged losses during unfavorable conditions. However, incorporating full flexibility—including both expansion and shutdown—further increases the expected NPV to $42.15 million. This $34.05 million difference represents the value of strategic adaptability, showcasing how proactive decision-making enhances project value.\nMuch like how military leaders develop contingency plans for unpredictable battle conditions, this approach enables management to adjust operations dynamically in response to market fluctuations. Instead of committing to a rigid, predetermined strategy, the company can mitigate downside risk by suspending operations when necessary while capitalizing on growth opportunities through expansion. This ability to optimize operations in real-time ensures the project remains resilient across varying market conditions, ultimately maximizing long-term value."
  },
  {
    "objectID": "projects/projects.html",
    "href": "projects/projects.html",
    "title": "projects",
    "section": "",
    "text": "#My projects\nHere are some projects that I’ve worked on:\n\nNPV at Risk Analysis\n\nFitness Assessment\n\nFixed Income Analysis\nGeneral Capabilities"
  },
  {
    "objectID": "projects/NPV_RISK_Analysis.html#historical-trend-distribution-analysis",
    "href": "projects/NPV_RISK_Analysis.html#historical-trend-distribution-analysis",
    "title": "A Desicion Towards Independence (NPV@RISK)",
    "section": "Historical Trend & Distribution Analysis",
    "text": "Historical Trend & Distribution Analysis\nIn November 2018, U.S. oil prices plunged below $50 per barrel, falling over 30% since October due to rising supply and fears of weakening demand. President Donald Trump praised Saudi Arabia for keeping prices low, while concerns grew over U.S. shale oversupply, OPEC production uncertainty, and geopolitical tensions. This market instability is reflected in the CL-SYN spread, which shows increased volatility during this period, highlighting the impact of global supply-demand shifts on regional crude pricing. (CNN)\nThe 2021–2023 global energy crisis was driven by post-pandemic supply shortages, coal trade disputes (China-Australia), climate-related hydropower declines, and geopolitical tensions like Russia’s invasion of Ukraine, which led to sanctions and disrupted oil and gas supplies. OPEC+ further tightened markets with production cuts in October 2022, intensifying price volatility and global energy shortages. This aligns with the CL-SYN spread surge in 2022, where prices spiked above $20/barrel, reflecting the market’s reaction to these supply constraints and geopolitical disruptions. (Wikipedia)\nThe CL-SYN spread distribution is right-skewed, with most values between CAD 10-15 per spread in barrels. With a skewness of 0.63 illustrating that majority of the historical observation has been towards the lower end of the spread. It means that majority of the time spread changes within lower end. Reflecting a typical premium for synthetic crude, this aligns with the observed mean of $16.47 per barrel, and extreme positives (up to CAD 35) align with major global events, such as the 2018 oil price crash, the 2020 COVID-19 collapse, and the 2021-2023 energy crisis, showing how external shocks drive volatility in the spread we are trying to capture.\nOverall, the CL-SYN spread shows signs of mean reversion, despite extreme deviations during global crises. Suggesting that while short-term shocks impact the spread, it tends to revert towards a stable range over time. Based on this analysis, we came to the conclusion to fit an Ornstein-Uhlenbeck Mean-reversion with Jumps model to effectively evaluate the value of our project decision."
  },
  {
    "objectID": "projects/NPV_RISK_Analysis.html#simulated-spread",
    "href": "projects/NPV_RISK_Analysis.html#simulated-spread",
    "title": "A Desicion Towards Independence (NPV@RISK)",
    "section": "Simulated Spread",
    "text": "Simulated Spread\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheta\nmu\nsigma\nhalfLife\njump_prob\njump_avesize\njump_stdv\n\n\n\n\n1.538397\n17.34466\n9.539241\n0.4505645\n0.0493827\n9.491148\n1.613076\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJustification of choice of parameters\nMu (Long-Run Mean): Our historical data exhibits a mean spread of 16.47, while our model estimates 17.34, showing a clear consistency with the observed data. This suggests our model effectively captures the central tendency of the spread.\nTheta (Mean-Reversion Speed): With a moderate theta of 1.54, the spread reverts to its mean approximately every 1.5 years. This reflects a realistic pace of reversion, balancing short-term fluctuations with long-term stability.\nSigma (Volatility): The estimated volatility of 9.53 suggests that while fluctuations occur, they are within a controlled range, capturing the moderate yet dynamic nature of the spread.\nJump Probability & Size: The model detects jumps occurring approximately 4.9% of the time. The average jump size is 9.49 dollars, aligning with observed market shocks, and the estimated jump standard deviation of 1.61 captures variability in jump magnitudes.\nHalf-Life: The spread takes approximately 0.45 years to close half the gap between its current level and the long-term mean. This estimation aligns well with market behavior, reinforcing the model’s ability to reflect real-world reversion dynamics.\nOverall, our Ornstein-Uhlenbeck with Jumps model appears to capture the spread’s mean-reverting nature while allowing for occasional significant deviations. The spread operates within a band of approximately -$8 to 42$/barrel, showing expected stochastic behavior with jumps, yet maintaining a stable long-term range, allowing our model to effectively account for uncertainty within the markets."
  },
  {
    "objectID": "projects/NPV_RISK_Analysis.html#npvrisk-analysis",
    "href": "projects/NPV_RISK_Analysis.html#npvrisk-analysis",
    "title": "A Desicion Towards Independence (NPV@RISK)",
    "section": "NPV@Risk Analysis",
    "text": "NPV@Risk Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNPV Risk Metrics\n\n\nMonte Carlo Simulation Results\n\n\nRisk Metric\nValue\n\n\n\n\nExpected NPV ($M)\n42.16\n\n\nStandard Deviation ($M)\n645.76\n\n\nVaR (95%) ($M)\n−1,016.74\n\n\nProbability of Negative NPV (%)\n48.60%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSensitivity Analysis\nWith 500 simulations for analysis, we get an expected (average) mean of $42 million. When increasing the simulation to 5000 the expected NPV decreased to -$47 million. With more simulations, the simulation distribution becomes more skewed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom our sensitivity analysis, we observe that the minimum operating threshold for operating cost should be 11 dollars per barrel (CL_SYN spread), because that is where the profit is maximized or expected NPV is highest. Also, the probability of negative NPV is 100% at the minimum operating threshold of $20 and lowest at $11. This solidifies the target minimum operating threshold to be $11.\nHigher expansion rate means expansion occurs more often in the simulations. Indicating that there is a positive relationship between higher expansion rate and higher NPV. However the sweet spot is around $20 to $25, that is, NPV changes the greatest and there is minimum change in expansion rate. A Higher expansion threshold lowers the expansion rate, and vice versa (as a result a lower NPV).\nAssuming that fixed costs and initial expenditure are predetermined or sunk cost we did not do a sensitivity analysis on them. It is with the assumption that the primary goal of a sensitivity analysis is to assess how changes in uncertain, variable factors—such as sales volume, selling price, or variable costs—affect the project’s outcomes. Since fixed costs and initial expenditures are usually contractual or one-time outlays, they’re assumed to remain unchanged unless we explicitly want to test scenarios where these assumptions might vary.\n\n\n\n\n\n\n\n\n\nThe tornado chart highlights that Operating Threshold (X) - Low has the most significant negative impact, driving NPV down by over $1.5 billion, as running operations at an unprofitable threshold leads to major losses. Expansion Threshold (Ci) - Low also reduces NPV, indicating that premature expansion increases costs without guaranteeing returns. Conversely, Expansion Threshold (Ci) - High is the only factor that increases NPV, showing that a more conservative expansion strategy results in better financial outcomes. These insights reinforce that setting an optimal Operating Threshold (X) is critical, as poor calibration can lead to extreme downside risk, while strategic expansion decisions help maximize profitability."
  },
  {
    "objectID": "projects/NPV_RISK_Analysis.html#strategic-recommendation",
    "href": "projects/NPV_RISK_Analysis.html#strategic-recommendation",
    "title": "A Desicion Towards Independence (NPV@RISK)",
    "section": "Strategic Recommendation",
    "text": "Strategic Recommendation\nBased on our analysis, we recommend proceeding with the project. However, there are clear opportunities to enhance profitability by refining key constraints identified in our evaluation.\nCurrently, the option to expand the upgrader is only available at t = 5, resembling a European call option. However, the existing expansion threshold of $25/barrel creates a significant limitation on potential returns. To maximize value, we recommend lowering this threshold to approximately $20/barrel, enabling optimal expansion and greater upside capture in favorable market conditions. This adjustment aligns with our NPV analysis and could significantly improve the project’s financial outcome.\n\n\n\nComparison of Traditional NPV vs. NPV@Risk\n\n\nMethod\nNPV Value ($M)\n\n\n\n\nTraditional NPV\n-55.34\n\n\nNPV@Risk (Mean)\n42.16\n\n\n\n\n\n\n\nThe Traditional NPV approach yields a negative value of -$55 million, suggesting that the project is not feasible under a rigid, static framework. In contrast, the NPV@Risk approach, which incorporates flexibility and uncertainty, results in a positive expected NPV of $42 million, demonstrating the significant value of adaptive decision-making. The differences between the two approaches highlight the clear limitations of a traditional NPV method, which fails to capture flexibility and changing market dynamics. While a traditional NPV approach can be served useful in stable, predictable environments (e.g regulated utilities), it struggles in more volatile markets (e.g commodities or energy), where price fluctuations and strategic adjustments play a more critical role. By incorporating NPV@Risk and real options analysis, firms can account for uncertainty, adaptability, and decision-making flexibility, ultimately leading to a more realistic valuation of their projects and more clear insights on the risks that come with it.\nBeyond the limitations of the traditional NPV approach, our analysis further explores how different levels of flexibility impact project value.\n\nThe value of operational flexibility and expansion\n\n\n\nExpected NPV and Option Value for different flexibility levels\n\n\nFlexibility\nExpected NPV ($M)\nOption Value ($M)\n\n\n\n\nFull Flexibility\n42.16\n247.69\n\n\nExpansion Only\n8.10\n34.06\n\n\nShutdown Flexibility\n-205.53\n213.63\n\n\n\n\n\n\n\nThe real options embedded in our NPV model highlight the substantial financial impact of managerial flexibility under uncertainty. Without any flexibility, where the project must always operate regardless of market conditions, the expected NPV is -$205.53 million, reflecting the significant downside risk. Allowing only the option to shut down (but not expand) improves this outcome dramatically to $8.10 million, demonstrating the importance of avoiding prolonged losses during unfavorable conditions. However, incorporating full flexibility—including both expansion and shutdown—further increases the expected NPV to $42.15 million. This $34.05 million difference represents the value of strategic adaptability, showcasing how proactive decision-making enhances project value.\nMuch like how military leaders develop contingency plans for unpredictable battle conditions, this approach enables management to adjust operations dynamically in response to market fluctuations. Instead of committing to a rigid, predetermined strategy, the company can mitigate downside risk by suspending operations when necessary while capitalizing on growth opportunities through expansion. This ability to optimize operations in real-time ensures the project remains resilient across varying market conditions, ultimately maximizing long-term value."
  },
  {
    "objectID": "projects/exam.html",
    "href": "projects/exam.html",
    "title": "FIN-450",
    "section": "",
    "text": "Extract the following data from FRED from 2014-01-01 to 2024-08-01.\n\nUnemployment Rate in Alaska.\nUnemployment Rate in District of Columbia.\n\nPerform the following:\n\nReturn a long data frame named unemployment with columns date, series and value.\nAmend the series names to AK and DC using tidyverse functions.\nUsing plotly, plot both time series on the same line chart. ggplot2 converted to plotly is not acceptable.\n\n\n# your code here\n\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(knitr)\nlibrary(gt)\n\ncountry &lt;- c(\"AKUR\", \"DCUR\")\n\nunemployment &lt;- country %&gt;% tidyquant::tq_get(get = \"economic.data\",\n                    from = \"2014-01-01\",\n                    to = \"2024-08-01\") %&gt;%\n  stats::na.omit()\n\nunemployment &lt;- unemployment %&gt;% \n  dplyr::mutate(series = stringr::str_replace_all(symbol, c(\"AKUR\" = \"AK\", \n                                                            \"DCUR\" = \"DC\")),\n                value = price) %&gt;%\n  dplyr::select(date, series, value) \n\nunemployment %&gt;% plotly::plot_ly(x = ~date, y = ~value, name = ~series, type = \"scatter\", mode = \"lines\")\n\n\n\n\n\n\n\n\n\n\n\nReturn a dataframe that shows the weight of each sector in the S&P 400.\nRound the weight to 3 decimals.\nSort the sector descending by weight.\n\n\n# hint: the output of your code should return a dataframe looking like the output of this one\nexample &lt;- dplyr::tibble(sector = c(\"Information Technology\",\"Consumer Discretionary\",\"Utilities\"),\n              weight = c(0.15,0.10,0.05))\n\nkable(example)\n\n\n\n\nsector\nweight\n\n\n\n\nInformation Technology\n0.15\n\n\nConsumer Discretionary\n0.10\n\n\nUtilities\n0.05\n\n\n\n\n\n\nlibrary(RTLedu)\nsp &lt;- sp400_desc\n# your code here\n\nweight &lt;- sp %&gt;% \n  dplyr::select(sector, weight) %&gt;%\n  dplyr::mutate(weight = round(weight, 3)) %&gt;%\n  dplyr::arrange(desc(weight))\nweight\n\n# A tibble: 401 × 2\n   sector                 weight\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 Health Care             0.008\n 2 Industrials             0.007\n 3 Industrials             0.007\n 4 Energy                  0.007\n 5 Industrials             0.007\n 6 Consumer Discretionary  0.006\n 7 Energy                  0.006\n 8 Materials               0.006\n 9 Information Technology  0.006\n10 Industrials             0.006\n# ℹ 391 more rows\n\n\n\n\n\nFor example, if each of the 10 largest weight companies had a weight of 1%, it would be 10%.\n\n# your code here\ntop15 &lt;- sp %&gt;%\n  select(company, weight) %&gt;%\n  dplyr::arrange(desc(weight)) %&gt;%\n  dplyr::slice(1:15) %&gt;%\n  dplyr::mutate(answer = cumsum(weight))\n\nanswer &lt;- top15 %&gt;% slice(15) %&gt;% select(answer)\ngt(answer)\n\n\n\n\n\n\n\nanswer\n\n\n\n\n0.09274607\n\n\n\n\n\n\n\n\n\n\nYou want to extract companies with the following criteria:\n\nThey are either in the Health Care OR Communication Services sector,\nAND they have a weight greater than 0.4%.\n\nCorrect the code I wrote which is not working…\n\n# leave this code as is and correct it in the next chunk\nsp400_desc %&gt;% tidyr::select(sector == \"Communication Services\" AND sector == \"Health Care\" OR weight &gt; 0.004)\n\n\n# Your corrected code here\ncorrected &lt;- sp400_desc %&gt;% dplyr::filter(sector == \"Communicaiton Services\" | sector == \"Health Care\", weight &gt; 0.004)\n\ngt(corrected)\n\n\n\n\n\n\n\nsymbol\ncompany\nidentifier\nsedol\nweight\nsector\nshares_held\nlocal_currency\n\n\n\n\nILMN\nIllumina Inc.\n452327109\n2613990\n0.007700409\nHealth Care\n1262275\nUSD\n\n\nAVTR\nAvantor Inc.\n05352A100\nBJLT387\n0.005408540\nHealth Care\n5386949\nUSD\n\n\nUTHR\nUnited Therapeutics Corporation\n91307C102\n2430412\n0.005326670\nHealth Care\n352542\nUSD\n\n\nTHC\nTenet Healthcare Corporation\n88033G407\nB8DMK08\n0.004949650\nHealth Care\n759273\nUSD\n\n\nBMRN\nBioMarin Pharmaceutical Inc.\n09061G101\n2437071\n0.004581570\nHealth Care\n1508557\nUSD\n\n\nSRPT\nSarepta Therapeutics Inc.\n803607100\nB8DPDT7\n0.004304370\nHealth Care\n755664\nUSD\n\n\n\n\n\n\n\n\n\n\n\nYou just graduated in Finance and took a job as an investment adviser for a company specializing in the real estate sector. Your company runs advertising portraying the benefit of the diversification it provides at all times versus equity indices.\nYou are skeptical.\n\nUse the following data set which represents prices of an ETF RealEstate and sp400.\nUse log() returns on for your analysis.\n\n\ncor &lt;- RTLedu::correlation %&gt;%\n  dplyr::group_by(series) %&gt;%\n  mutate(log_return = log(value / dplyr::lag(value))) %&gt;%\n  tidyr::drop_na(log_return)\n\n\n\n\n# your code here\n\ncor.roll &lt;- cor %&gt;%\n  dplyr::select(date, series, log_return) %&gt;%\n  tidyr::pivot_wider(names_from = series, values_from = log_return) %&gt;%\n  dplyr::mutate(cor60 = slider::pslide_dbl(\n    .l = list(RealEstate, sp400),\n    .f = ~ cor(.x, .y),\n    .before = 60,\n    .after = 0,\n    .complete = TRUE\n  )) %&gt;%\n  tidyr::drop_na()\n\ncor.roll %&gt;%\n  ggplot(aes(x = date, y = cor60)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"60-day Rolling Correlation\", x = \"\", y = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\nPre COVID19: 2017-2019.\nPost COVID19: 2020-now.\n\nFor full points, you must create a variable in your dataframe using dplyr::mutate() with the pre and post correlation periods (tidy workflow).\n\n# your code here\nroll_cor &lt;- cor.roll %&gt;%\n  dplyr::mutate(periods = dplyr::if_else(date &lt; \"2020-01-01\", \"Pre-COVID19\", \"Post-COVID19\")) %&gt;%\n  group_by(periods) %&gt;%\n  dplyr::summarise(avg_roll_cor = mean(cor60))\n\nkable(roll_cor, digits = 3)\n\n\n\n\nperiods\navg_roll_cor\n\n\n\n\nPost-COVID19\n0.687\n\n\nPre-COVID19\n0.490\n\n\n\n\n\n\nPre-COVID19 = 0.49, Post-COVID19 = 0.69\nPre-COVID19 = 0.52, Post-COVID19 = 0.69\nPre-COVID19 = 0.49, Post-COVID19 = 0.81\nPre-COVID19 = 0.52, Post-COVID19 = 0.81\nPre-COVID19 = 0.49, Post-COVID19 = 0.687 = 0.69\n\nyour answer\nA\n\n\n\n\nUsing the RTLedu::unemployment data set:\n\n\nIn the code chunk below: Use the feast::STL() model and plot the results using fabletools::components().\nAdd a short paragraph telling me what you observe in the change over time in their seasonality patterns.\nFrom the chart we can see that the unemployment rates in each state seem to follow a similar trend in all aspects, with Alaska having a more subtle increase and decrease in rates throughout the years, we can also see that California has the most distinct increases and decreases over the years. Observing seasonality, we can see that Alaska has the most noticeable seasonality patterns throughout the years with an interesting decrease overtime, while California and New Jersey have had a very similar pattern of seasonality. Interestingly, California and New Jersey have had an increase in seasonaly patterns throughout the years. The STL decomposition shows the effects of economic changes throughout the years through analysis of the remainder, where there was a significant jump in the unemployment rate beginning 2020 (COVID-19).\n\n# your code here\nseas &lt;- RTLedu::unemployment\n\nlibrary(fabletools)\nlibrary(feasts)\nlibrary(tsibble)\n\nseas_tsi &lt;- seas %&gt;%\n  tsibble::as_tsibble(key = state, index = date) %&gt;%\n  tsibble::index_by(freq = ~yearmonth(.)) %&gt;%\n  tsibble::group_by_key() %&gt;%\n  dplyr::summarise(\n    rate = mean(rate),\n    .groups = \"keep\"\n  ) %&gt;%\n  stats::na.omit()\n\nstl&lt;- seas_tsi %&gt;%\n  fabletools::model(feasts::STL(formula = rate ~ season(window = 13)))\n\nstl %&gt;% fabletools::components() %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\n\n\n# your code here\n\nstr_stats &lt;- seas_tsi %&gt;%\n  fabletools::features(rate, feasts::feat_stl)\nkable(str_stats, digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstate\ntrend_strength\nseasonal_strength_year\nseasonal_peak_year\nseasonal_trough_year\nspikiness\nlinearity\ncurvature\nstl_e_acf1\nstl_e_acf10\n\n\n\n\nAlaska\n0.856\n0.656\n2\n8\n0\n-0.097\n-0.084\n0.667\n0.978\n\n\nCalifornia\n0.931\n0.236\n4\n11\n0\n-0.064\n-0.204\n0.670\n0.818\n\n\nNewJersey\n0.877\n0.281\n7\n11\n0\n-0.012\n-0.165\n0.677\n0.982\n\n\n\n\n\n\n\n\n\n\n\nThis question will use the RTLedu::reg3 data set where:\n\nICLN is a clean energy ETF.\nXLE is the Energy industry ETF of the sp500 index.\nYou own ICLN in your portfolio.\nYour are interested in understanding how XLE returns explain ICLN returns.\nNo residuals or ACF tests are required for this question.\n\n\nreg1 &lt;- RTLedu::reg3\n# your code here\nlibrary(broom)\n\nfit &lt;- stats::lm(ICLN ~ XLE, reg1)\nmodel_fit &lt;- broom::tidy(fit)\n\nkable(model_fit, digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.000\n0.000\n0.453\n0.651\n\n\nXLE\n0.437\n0.016\n27.879\n0.000\n\n\n\n\nhedge_ratio &lt;- cor(reg1$ICLN, reg1$XLE) * (sd(reg1$ICLN) / sd(reg1$XLE))\n\n\nThe regression and beta (coefficient estimate) are significant.\nThe beta (coefficient estimate) is significant and the regression is not.\nTo hedge your thousand dollar investment in ICLN, you should sell approximately $450 of XLE shares.\nTo hedge your thousand dollar investment in ICLN, you should sell approximately $550 of XLE shares.\n\nyour answer(s)\nB, C\n\n\n\nA work colleague has done the regression shown below.\nHer boss knows you have a Finance background and asking you for your critical opinion.\nWrite a few bullet points summarizing your conclusions.\n\nThe Coefficient X is significant and the model explains 76.31% of variability, however the residuals tell another story.\nWe can see from the residuals vs fitted graph that there is a curved pattern, which shows clear non-linearity, meaning that the model might not fully capture the true relationship of the data, I would suggest a non-linear model, perhaps a cubic function of some sort.\nFrom the Normal Q-Q graph we can see that the residuals are not normally distributed at least near the tails of the residuals.\nThere are present observations that may be influencing the model (82, 209, 20, 19), however this should be analyzed after transforming the model.\nFrom the Breusch-Pagan Test, we can see that heteroscedasticity is present\n\n\nlibrary(ggfortify)\nreg &lt;- lm(y ~ x,data = RTLedu::reg2)\n\n\n\nRTLedu::reg2 %&gt;% ggplot(aes(x = RTLedu::reg2$x, y = RTLedu::reg2$y)) +\n  geom_point(alpha = 0.6, color = \"blue\") +\n  labs(title = \"Scatterplot of our Data\", x = \"x\", y = \"y\") +\n  theme_minimal()\n\n\n\n\n\n\n\nresults &lt;- summary(reg)\nmodel_results &lt;- broom::tidy(results)\nkable(model_results, digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.461\n0.277\n1.664\n0.099\n\n\nx\n0.677\n0.038\n17.770\n0.000\n\n\n\n\nautoplot(reg, size =0.5)\n\n\n\n\n\n\n\ntest_results &lt;- lmtest::bgtest(fit) %&gt;% broom::tidy()\nkable(test_results)\n\n\n\n\n\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n\n\n5.224175\n0.022275\n1\nBreusch-Godfrey test for serial correlation of order up to 1"
  },
  {
    "objectID": "projects/exam.html#questions",
    "href": "projects/exam.html#questions",
    "title": "FIN-450",
    "section": "",
    "text": "Extract the following data from FRED from 2014-01-01 to 2024-08-01.\n\nUnemployment Rate in Alaska.\nUnemployment Rate in District of Columbia.\n\nPerform the following:\n\nReturn a long data frame named unemployment with columns date, series and value.\nAmend the series names to AK and DC using tidyverse functions.\nUsing plotly, plot both time series on the same line chart. ggplot2 converted to plotly is not acceptable.\n\n\n# your code here\n\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(knitr)\nlibrary(gt)\n\ncountry &lt;- c(\"AKUR\", \"DCUR\")\n\nunemployment &lt;- country %&gt;% tidyquant::tq_get(get = \"economic.data\",\n                    from = \"2014-01-01\",\n                    to = \"2024-08-01\") %&gt;%\n  stats::na.omit()\n\nunemployment &lt;- unemployment %&gt;% \n  dplyr::mutate(series = stringr::str_replace_all(symbol, c(\"AKUR\" = \"AK\", \n                                                            \"DCUR\" = \"DC\")),\n                value = price) %&gt;%\n  dplyr::select(date, series, value) \n\nunemployment %&gt;% plotly::plot_ly(x = ~date, y = ~value, name = ~series, type = \"scatter\", mode = \"lines\")\n\n\n\n\n\n\n\n\n\n\n\nReturn a dataframe that shows the weight of each sector in the S&P 400.\nRound the weight to 3 decimals.\nSort the sector descending by weight.\n\n\n# hint: the output of your code should return a dataframe looking like the output of this one\nexample &lt;- dplyr::tibble(sector = c(\"Information Technology\",\"Consumer Discretionary\",\"Utilities\"),\n              weight = c(0.15,0.10,0.05))\n\nkable(example)\n\n\n\n\nsector\nweight\n\n\n\n\nInformation Technology\n0.15\n\n\nConsumer Discretionary\n0.10\n\n\nUtilities\n0.05\n\n\n\n\n\n\nlibrary(RTLedu)\nsp &lt;- sp400_desc\n# your code here\n\nweight &lt;- sp %&gt;% \n  dplyr::select(sector, weight) %&gt;%\n  dplyr::mutate(weight = round(weight, 3)) %&gt;%\n  dplyr::arrange(desc(weight))\nweight\n\n# A tibble: 401 × 2\n   sector                 weight\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 Health Care             0.008\n 2 Industrials             0.007\n 3 Industrials             0.007\n 4 Energy                  0.007\n 5 Industrials             0.007\n 6 Consumer Discretionary  0.006\n 7 Energy                  0.006\n 8 Materials               0.006\n 9 Information Technology  0.006\n10 Industrials             0.006\n# ℹ 391 more rows\n\n\n\n\n\nFor example, if each of the 10 largest weight companies had a weight of 1%, it would be 10%.\n\n# your code here\ntop15 &lt;- sp %&gt;%\n  select(company, weight) %&gt;%\n  dplyr::arrange(desc(weight)) %&gt;%\n  dplyr::slice(1:15) %&gt;%\n  dplyr::mutate(answer = cumsum(weight))\n\nanswer &lt;- top15 %&gt;% slice(15) %&gt;% select(answer)\ngt(answer)\n\n\n\n\n\n\n\nanswer\n\n\n\n\n0.09274607\n\n\n\n\n\n\n\n\n\n\nYou want to extract companies with the following criteria:\n\nThey are either in the Health Care OR Communication Services sector,\nAND they have a weight greater than 0.4%.\n\nCorrect the code I wrote which is not working…\n\n# leave this code as is and correct it in the next chunk\nsp400_desc %&gt;% tidyr::select(sector == \"Communication Services\" AND sector == \"Health Care\" OR weight &gt; 0.004)\n\n\n# Your corrected code here\ncorrected &lt;- sp400_desc %&gt;% dplyr::filter(sector == \"Communicaiton Services\" | sector == \"Health Care\", weight &gt; 0.004)\n\ngt(corrected)\n\n\n\n\n\n\n\nsymbol\ncompany\nidentifier\nsedol\nweight\nsector\nshares_held\nlocal_currency\n\n\n\n\nILMN\nIllumina Inc.\n452327109\n2613990\n0.007700409\nHealth Care\n1262275\nUSD\n\n\nAVTR\nAvantor Inc.\n05352A100\nBJLT387\n0.005408540\nHealth Care\n5386949\nUSD\n\n\nUTHR\nUnited Therapeutics Corporation\n91307C102\n2430412\n0.005326670\nHealth Care\n352542\nUSD\n\n\nTHC\nTenet Healthcare Corporation\n88033G407\nB8DMK08\n0.004949650\nHealth Care\n759273\nUSD\n\n\nBMRN\nBioMarin Pharmaceutical Inc.\n09061G101\n2437071\n0.004581570\nHealth Care\n1508557\nUSD\n\n\nSRPT\nSarepta Therapeutics Inc.\n803607100\nB8DPDT7\n0.004304370\nHealth Care\n755664\nUSD\n\n\n\n\n\n\n\n\n\n\n\nYou just graduated in Finance and took a job as an investment adviser for a company specializing in the real estate sector. Your company runs advertising portraying the benefit of the diversification it provides at all times versus equity indices.\nYou are skeptical.\n\nUse the following data set which represents prices of an ETF RealEstate and sp400.\nUse log() returns on for your analysis.\n\n\ncor &lt;- RTLedu::correlation %&gt;%\n  dplyr::group_by(series) %&gt;%\n  mutate(log_return = log(value / dplyr::lag(value))) %&gt;%\n  tidyr::drop_na(log_return)\n\n\n\n\n# your code here\n\ncor.roll &lt;- cor %&gt;%\n  dplyr::select(date, series, log_return) %&gt;%\n  tidyr::pivot_wider(names_from = series, values_from = log_return) %&gt;%\n  dplyr::mutate(cor60 = slider::pslide_dbl(\n    .l = list(RealEstate, sp400),\n    .f = ~ cor(.x, .y),\n    .before = 60,\n    .after = 0,\n    .complete = TRUE\n  )) %&gt;%\n  tidyr::drop_na()\n\ncor.roll %&gt;%\n  ggplot(aes(x = date, y = cor60)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"60-day Rolling Correlation\", x = \"\", y = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\nPre COVID19: 2017-2019.\nPost COVID19: 2020-now.\n\nFor full points, you must create a variable in your dataframe using dplyr::mutate() with the pre and post correlation periods (tidy workflow).\n\n# your code here\nroll_cor &lt;- cor.roll %&gt;%\n  dplyr::mutate(periods = dplyr::if_else(date &lt; \"2020-01-01\", \"Pre-COVID19\", \"Post-COVID19\")) %&gt;%\n  group_by(periods) %&gt;%\n  dplyr::summarise(avg_roll_cor = mean(cor60))\n\nkable(roll_cor, digits = 3)\n\n\n\n\nperiods\navg_roll_cor\n\n\n\n\nPost-COVID19\n0.687\n\n\nPre-COVID19\n0.490\n\n\n\n\n\n\nPre-COVID19 = 0.49, Post-COVID19 = 0.69\nPre-COVID19 = 0.52, Post-COVID19 = 0.69\nPre-COVID19 = 0.49, Post-COVID19 = 0.81\nPre-COVID19 = 0.52, Post-COVID19 = 0.81\nPre-COVID19 = 0.49, Post-COVID19 = 0.687 = 0.69\n\nyour answer\nA\n\n\n\n\nUsing the RTLedu::unemployment data set:\n\n\nIn the code chunk below: Use the feast::STL() model and plot the results using fabletools::components().\nAdd a short paragraph telling me what you observe in the change over time in their seasonality patterns.\nFrom the chart we can see that the unemployment rates in each state seem to follow a similar trend in all aspects, with Alaska having a more subtle increase and decrease in rates throughout the years, we can also see that California has the most distinct increases and decreases over the years. Observing seasonality, we can see that Alaska has the most noticeable seasonality patterns throughout the years with an interesting decrease overtime, while California and New Jersey have had a very similar pattern of seasonality. Interestingly, California and New Jersey have had an increase in seasonaly patterns throughout the years. The STL decomposition shows the effects of economic changes throughout the years through analysis of the remainder, where there was a significant jump in the unemployment rate beginning 2020 (COVID-19).\n\n# your code here\nseas &lt;- RTLedu::unemployment\n\nlibrary(fabletools)\nlibrary(feasts)\nlibrary(tsibble)\n\nseas_tsi &lt;- seas %&gt;%\n  tsibble::as_tsibble(key = state, index = date) %&gt;%\n  tsibble::index_by(freq = ~yearmonth(.)) %&gt;%\n  tsibble::group_by_key() %&gt;%\n  dplyr::summarise(\n    rate = mean(rate),\n    .groups = \"keep\"\n  ) %&gt;%\n  stats::na.omit()\n\nstl&lt;- seas_tsi %&gt;%\n  fabletools::model(feasts::STL(formula = rate ~ season(window = 13)))\n\nstl %&gt;% fabletools::components() %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\n\n\n# your code here\n\nstr_stats &lt;- seas_tsi %&gt;%\n  fabletools::features(rate, feasts::feat_stl)\nkable(str_stats, digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstate\ntrend_strength\nseasonal_strength_year\nseasonal_peak_year\nseasonal_trough_year\nspikiness\nlinearity\ncurvature\nstl_e_acf1\nstl_e_acf10\n\n\n\n\nAlaska\n0.856\n0.656\n2\n8\n0\n-0.097\n-0.084\n0.667\n0.978\n\n\nCalifornia\n0.931\n0.236\n4\n11\n0\n-0.064\n-0.204\n0.670\n0.818\n\n\nNewJersey\n0.877\n0.281\n7\n11\n0\n-0.012\n-0.165\n0.677\n0.982\n\n\n\n\n\n\n\n\n\n\n\nThis question will use the RTLedu::reg3 data set where:\n\nICLN is a clean energy ETF.\nXLE is the Energy industry ETF of the sp500 index.\nYou own ICLN in your portfolio.\nYour are interested in understanding how XLE returns explain ICLN returns.\nNo residuals or ACF tests are required for this question.\n\n\nreg1 &lt;- RTLedu::reg3\n# your code here\nlibrary(broom)\n\nfit &lt;- stats::lm(ICLN ~ XLE, reg1)\nmodel_fit &lt;- broom::tidy(fit)\n\nkable(model_fit, digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.000\n0.000\n0.453\n0.651\n\n\nXLE\n0.437\n0.016\n27.879\n0.000\n\n\n\n\nhedge_ratio &lt;- cor(reg1$ICLN, reg1$XLE) * (sd(reg1$ICLN) / sd(reg1$XLE))\n\n\nThe regression and beta (coefficient estimate) are significant.\nThe beta (coefficient estimate) is significant and the regression is not.\nTo hedge your thousand dollar investment in ICLN, you should sell approximately $450 of XLE shares.\nTo hedge your thousand dollar investment in ICLN, you should sell approximately $550 of XLE shares.\n\nyour answer(s)\nB, C\n\n\n\nA work colleague has done the regression shown below.\nHer boss knows you have a Finance background and asking you for your critical opinion.\nWrite a few bullet points summarizing your conclusions.\n\nThe Coefficient X is significant and the model explains 76.31% of variability, however the residuals tell another story.\nWe can see from the residuals vs fitted graph that there is a curved pattern, which shows clear non-linearity, meaning that the model might not fully capture the true relationship of the data, I would suggest a non-linear model, perhaps a cubic function of some sort.\nFrom the Normal Q-Q graph we can see that the residuals are not normally distributed at least near the tails of the residuals.\nThere are present observations that may be influencing the model (82, 209, 20, 19), however this should be analyzed after transforming the model.\nFrom the Breusch-Pagan Test, we can see that heteroscedasticity is present\n\n\nlibrary(ggfortify)\nreg &lt;- lm(y ~ x,data = RTLedu::reg2)\n\n\n\nRTLedu::reg2 %&gt;% ggplot(aes(x = RTLedu::reg2$x, y = RTLedu::reg2$y)) +\n  geom_point(alpha = 0.6, color = \"blue\") +\n  labs(title = \"Scatterplot of our Data\", x = \"x\", y = \"y\") +\n  theme_minimal()\n\n\n\n\n\n\n\nresults &lt;- summary(reg)\nmodel_results &lt;- broom::tidy(results)\nkable(model_results, digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.461\n0.277\n1.664\n0.099\n\n\nx\n0.677\n0.038\n17.770\n0.000\n\n\n\n\nautoplot(reg, size =0.5)\n\n\n\n\n\n\n\ntest_results &lt;- lmtest::bgtest(fit) %&gt;% broom::tidy()\nkable(test_results)\n\n\n\n\n\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n\n\n5.224175\n0.022275\n1\nBreusch-Godfrey test for serial correlation of order up to 1"
  }
  {
  "objectID": "about.html",
  "href": "about.html",
  "title": "Resumes",
  "section": "",
  "text": "#My Resumes\nHere are some of my resumes and CVs:\n\nProfessional Resume\n\nAcademic CV\n\nTechnical Resume"
}
]